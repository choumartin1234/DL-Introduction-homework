{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-1: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.15\n",
    "weight_decay = 0.0001\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 8.9285\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 4.9468\t Accuracy 0.1094\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.7619\t Accuracy 0.1063\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.0176\t Accuracy 0.1045\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.6509\t Accuracy 0.1058\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.4207\t Accuracy 0.1045\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.2679\t Accuracy 0.1041\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.1561\t Accuracy 0.1048\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.0721\t Accuracy 0.1052\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.0059\t Accuracy 0.1069\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.9548\t Accuracy 0.1093\n",
      "\n",
      "Epoch [0]\t Average training loss 0.9118\t Average training accuracy 0.1141\n",
      "Epoch [0]\t Average validation loss 0.4579\t Average validation accuracy 0.2024\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4463\t Accuracy 0.2300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4495\t Accuracy 0.2286\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4281\t Accuracy 0.2560\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4130\t Accuracy 0.2700\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4065\t Accuracy 0.2819\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4002\t Accuracy 0.2876\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3965\t Accuracy 0.2918\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3927\t Accuracy 0.2957\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3903\t Accuracy 0.2983\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3876\t Accuracy 0.3021\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3871\t Accuracy 0.3045\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3858\t Average training accuracy 0.3056\n",
      "Epoch [1]\t Average validation loss 0.3606\t Average validation accuracy 0.3230\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3579\t Accuracy 0.3600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3638\t Accuracy 0.3461\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3641\t Accuracy 0.3452\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3636\t Accuracy 0.3482\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3638\t Accuracy 0.3564\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3605\t Accuracy 0.3649\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3562\t Accuracy 0.3738\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3506\t Accuracy 0.3878\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3447\t Accuracy 0.4046\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3389\t Accuracy 0.4203\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3355\t Accuracy 0.4301\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3320\t Average training accuracy 0.4366\n",
      "Epoch [2]\t Average validation loss 0.2832\t Average validation accuracy 0.5316\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2878\t Accuracy 0.5700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.2859\t Accuracy 0.5143\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.2875\t Accuracy 0.5128\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.2884\t Accuracy 0.5086\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.2896\t Accuracy 0.5123\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.2891\t Accuracy 0.5114\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.2894\t Accuracy 0.5099\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.2892\t Accuracy 0.5096\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.2891\t Accuracy 0.5099\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.2884\t Accuracy 0.5112\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.2891\t Accuracy 0.5111\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2891\t Average training accuracy 0.5091\n",
      "Epoch [3]\t Average validation loss 0.2778\t Average validation accuracy 0.5302\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2820\t Accuracy 0.6000\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2797\t Accuracy 0.5106\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2818\t Accuracy 0.5059\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2831\t Accuracy 0.5013\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2841\t Accuracy 0.5061\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2838\t Accuracy 0.5059\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2843\t Accuracy 0.5046\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2842\t Accuracy 0.5044\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2843\t Accuracy 0.5049\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2838\t Accuracy 0.5058\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2844\t Accuracy 0.5060\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2845\t Average training accuracy 0.5045\n",
      "Epoch [4]\t Average validation loss 0.2749\t Average validation accuracy 0.5496\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2799\t Accuracy 0.5900\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2764\t Accuracy 0.5129\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2787\t Accuracy 0.5098\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2799\t Accuracy 0.5055\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2807\t Accuracy 0.5097\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2804\t Accuracy 0.5114\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2806\t Accuracy 0.5132\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2798\t Accuracy 0.5179\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2777\t Accuracy 0.5255\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2746\t Accuracy 0.5326\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2727\t Accuracy 0.5389\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2704\t Average training accuracy 0.5434\n",
      "Epoch [5]\t Average validation loss 0.2380\t Average validation accuracy 0.6228\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2349\t Accuracy 0.6700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2394\t Accuracy 0.6000\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2403\t Accuracy 0.5964\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2416\t Accuracy 0.5920\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2421\t Accuracy 0.5958\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2419\t Accuracy 0.5951\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2423\t Accuracy 0.5929\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2423\t Accuracy 0.5925\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2422\t Accuracy 0.5925\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2417\t Accuracy 0.5929\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2420\t Accuracy 0.5937\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2418\t Average training accuracy 0.5932\n",
      "Epoch [6]\t Average validation loss 0.2317\t Average validation accuracy 0.6126\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2264\t Accuracy 0.6900\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2338\t Accuracy 0.6025\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2349\t Accuracy 0.5997\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2365\t Accuracy 0.5950\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2370\t Accuracy 0.5973\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2371\t Accuracy 0.5962\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2377\t Accuracy 0.5941\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2378\t Accuracy 0.5936\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2379\t Accuracy 0.5941\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2374\t Accuracy 0.5947\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2378\t Accuracy 0.5961\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2377\t Average training accuracy 0.5957\n",
      "Epoch [7]\t Average validation loss 0.2288\t Average validation accuracy 0.6196\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2225\t Accuracy 0.7000\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2306\t Accuracy 0.6067\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2314\t Accuracy 0.6073\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2327\t Accuracy 0.6035\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2330\t Accuracy 0.6097\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2323\t Accuracy 0.6133\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2319\t Accuracy 0.6207\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2311\t Accuracy 0.6269\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2303\t Accuracy 0.6327\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2287\t Accuracy 0.6394\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2281\t Accuracy 0.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.2270\t Average training accuracy 0.6486\n",
      "Epoch [8]\t Average validation loss 0.2047\t Average validation accuracy 0.7452\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2108\t Accuracy 0.7600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2078\t Accuracy 0.7104\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2078\t Accuracy 0.7157\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2071\t Accuracy 0.7197\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2050\t Accuracy 0.7300\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2020\t Accuracy 0.7351\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1995\t Accuracy 0.7397\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1977\t Accuracy 0.7434\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1958\t Accuracy 0.7459\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1937\t Accuracy 0.7484\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1927\t Accuracy 0.7500\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1913\t Average training accuracy 0.7502\n",
      "Epoch [9]\t Average validation loss 0.1641\t Average validation accuracy 0.7834\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1734\t Accuracy 0.7900\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.1712\t Accuracy 0.7645\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1711\t Accuracy 0.7678\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1733\t Accuracy 0.7636\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1738\t Accuracy 0.7674\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1733\t Accuracy 0.7663\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1732\t Accuracy 0.7665\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1736\t Accuracy 0.7666\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1736\t Accuracy 0.7663\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1730\t Accuracy 0.7676\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1732\t Accuracy 0.7674\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1729\t Average training accuracy 0.7670\n",
      "Epoch [10]\t Average validation loss 0.1579\t Average validation accuracy 0.7746\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1651\t Accuracy 0.7800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1649\t Accuracy 0.7676\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.1650\t Accuracy 0.7689\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1674\t Accuracy 0.7614\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1680\t Accuracy 0.7639\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1676\t Accuracy 0.7629\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1676\t Accuracy 0.7649\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1680\t Accuracy 0.7679\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1681\t Accuracy 0.7706\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1673\t Accuracy 0.7738\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1667\t Accuracy 0.7781\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1653\t Average training accuracy 0.7814\n",
      "Epoch [11]\t Average validation loss 0.1368\t Average validation accuracy 0.8314\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1365\t Accuracy 0.8600\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1448\t Accuracy 0.8137\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1427\t Accuracy 0.8246\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1446\t Accuracy 0.8194\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.1440\t Accuracy 0.8237\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1429\t Accuracy 0.8248\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1426\t Accuracy 0.8252\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.1429\t Accuracy 0.8247\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1427\t Accuracy 0.8244\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1421\t Accuracy 0.8250\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1420\t Accuracy 0.8251\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1415\t Average training accuracy 0.8250\n",
      "Epoch [12]\t Average validation loss 0.1221\t Average validation accuracy 0.8400\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1247\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.1329\t Accuracy 0.8227\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1320\t Accuracy 0.8331\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1349\t Accuracy 0.8268\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1351\t Accuracy 0.8296\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1346\t Accuracy 0.8295\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1348\t Accuracy 0.8296\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1354\t Accuracy 0.8290\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1356\t Accuracy 0.8279\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.1354\t Accuracy 0.8285\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.1356\t Accuracy 0.8283\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1353\t Average training accuracy 0.8281\n",
      "Epoch [13]\t Average validation loss 0.1189\t Average validation accuracy 0.8406\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1217\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.1297\t Accuracy 0.8255\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.1289\t Accuracy 0.8331\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1318\t Accuracy 0.8274\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1321\t Accuracy 0.8301\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1316\t Accuracy 0.8304\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1318\t Accuracy 0.8303\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1326\t Accuracy 0.8297\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1328\t Accuracy 0.8289\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1327\t Accuracy 0.8294\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1329\t Accuracy 0.8293\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1327\t Average training accuracy 0.8292\n",
      "Epoch [14]\t Average validation loss 0.1171\t Average validation accuracy 0.8410\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1199\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.1277\t Accuracy 0.8276\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.1269\t Accuracy 0.8338\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.1299\t Accuracy 0.8285\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.1301\t Accuracy 0.8313\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.1297\t Accuracy 0.8314\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.1299\t Accuracy 0.8314\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.1307\t Accuracy 0.8309\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.1309\t Accuracy 0.8300\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.1309\t Accuracy 0.8302\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.1311\t Accuracy 0.8302\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1309\t Average training accuracy 0.8301\n",
      "Epoch [15]\t Average validation loss 0.1158\t Average validation accuracy 0.8414\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1185\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.1262\t Accuracy 0.8288\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.1255\t Accuracy 0.8348\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.1284\t Accuracy 0.8297\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.1286\t Accuracy 0.8320\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.1282\t Accuracy 0.8322\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.1285\t Accuracy 0.8322\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.1292\t Accuracy 0.8319\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.1295\t Accuracy 0.8314\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.1294\t Accuracy 0.8318\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.1296\t Accuracy 0.8318\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1294\t Average training accuracy 0.8315\n",
      "Epoch [16]\t Average validation loss 0.1147\t Average validation accuracy 0.8422\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1171\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.1249\t Accuracy 0.8294\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.1241\t Accuracy 0.8367\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.1271\t Accuracy 0.8327\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.1273\t Accuracy 0.8348\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.1269\t Accuracy 0.8347\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.1271\t Accuracy 0.8353\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.1278\t Accuracy 0.8350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.1280\t Accuracy 0.8352\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.1279\t Accuracy 0.8362\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.1281\t Accuracy 0.8366\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1278\t Average training accuracy 0.8375\n",
      "Epoch [17]\t Average validation loss 0.1115\t Average validation accuracy 0.8418\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1125\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.1207\t Accuracy 0.8533\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.1192\t Accuracy 0.8653\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.1212\t Accuracy 0.8638\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.1199\t Accuracy 0.8678\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.1181\t Accuracy 0.8712\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.1167\t Accuracy 0.8745\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.1159\t Accuracy 0.8761\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.1149\t Accuracy 0.8779\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.1138\t Accuracy 0.8792\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.1130\t Accuracy 0.8805\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1119\t Average training accuracy 0.8819\n",
      "Epoch [18]\t Average validation loss 0.0840\t Average validation accuracy 0.9218\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0834\t Accuracy 0.9400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0929\t Accuracy 0.9096\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0937\t Accuracy 0.9061\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0968\t Accuracy 0.9013\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0965\t Accuracy 0.9012\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0963\t Accuracy 0.9013\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0963\t Accuracy 0.9011\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0968\t Accuracy 0.9004\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0970\t Accuracy 0.8999\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0971\t Accuracy 0.8994\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0973\t Accuracy 0.8990\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0971\t Average training accuracy 0.8992\n",
      "Epoch [19]\t Average validation loss 0.0790\t Average validation accuracy 0.9248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9061.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.2910\t Accuracy 0.1400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.9569\t Accuracy 0.4108\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.6010\t Accuracy 0.5569\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.4695\t Accuracy 0.6282\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.3941\t Accuracy 0.6791\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.3449\t Accuracy 0.7147\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.3099\t Accuracy 0.7407\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.2839\t Accuracy 0.7603\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.2634\t Accuracy 0.7769\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.2469\t Accuracy 0.7901\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.2336\t Accuracy 0.8001\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2222\t Average training accuracy 0.8091\n",
      "Epoch [0]\t Average validation loss 0.0903\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0899\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0982\t Accuracy 0.9161\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0996\t Accuracy 0.9123\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1021\t Accuracy 0.9098\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1004\t Accuracy 0.9126\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0997\t Accuracy 0.9129\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0994\t Accuracy 0.9133\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0992\t Accuracy 0.9130\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0991\t Accuracy 0.9133\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0987\t Accuracy 0.9138\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0986\t Accuracy 0.9135\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0979\t Average training accuracy 0.9137\n",
      "Epoch [1]\t Average validation loss 0.0815\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0805\t Accuracy 0.9600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0858\t Accuracy 0.9290\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0880\t Accuracy 0.9244\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0909\t Accuracy 0.9206\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0895\t Accuracy 0.9229\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0890\t Accuracy 0.9239\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0891\t Accuracy 0.9233\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0891\t Accuracy 0.9230\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0893\t Accuracy 0.9231\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0891\t Accuracy 0.9234\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0892\t Accuracy 0.9232\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0888\t Average training accuracy 0.9232\n",
      "Epoch [2]\t Average validation loss 0.0770\t Average validation accuracy 0.9454\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0734\t Accuracy 0.9600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0799\t Accuracy 0.9333\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0820\t Accuracy 0.9304\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0847\t Accuracy 0.9275\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0835\t Accuracy 0.9296\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0830\t Accuracy 0.9304\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0833\t Accuracy 0.9298\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0833\t Accuracy 0.9292\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0836\t Accuracy 0.9294\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0834\t Accuracy 0.9297\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0837\t Accuracy 0.9292\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0834\t Average training accuracy 0.9290\n",
      "Epoch [3]\t Average validation loss 0.0739\t Average validation accuracy 0.9486\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0693\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0760\t Accuracy 0.9375\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0781\t Accuracy 0.9350\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0807\t Accuracy 0.9325\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0795\t Accuracy 0.9344\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0791\t Accuracy 0.9347\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0794\t Accuracy 0.9343\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0794\t Accuracy 0.9339\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0797\t Accuracy 0.9338\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0796\t Accuracy 0.9341\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0798\t Accuracy 0.9336\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0796\t Average training accuracy 0.9333\n",
      "Epoch [4]\t Average validation loss 0.0716\t Average validation accuracy 0.9518\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0668\t Accuracy 0.9600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0732\t Accuracy 0.9404\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0751\t Accuracy 0.9388\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0775\t Accuracy 0.9363\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0765\t Accuracy 0.9378\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0761\t Accuracy 0.9384\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0763\t Accuracy 0.9377\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0764\t Accuracy 0.9372\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0767\t Accuracy 0.9370\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0766\t Accuracy 0.9373\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0769\t Accuracy 0.9367\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0767\t Average training accuracy 0.9364\n",
      "Epoch [5]\t Average validation loss 0.0696\t Average validation accuracy 0.9520\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0654\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0707\t Accuracy 0.9445\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0725\t Accuracy 0.9438\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0748\t Accuracy 0.9405\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0739\t Accuracy 0.9417\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0734\t Accuracy 0.9422\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0738\t Accuracy 0.9413\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0738\t Accuracy 0.9411\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0741\t Accuracy 0.9405\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0740\t Accuracy 0.9406\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0743\t Accuracy 0.9399\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0742\t Average training accuracy 0.9397\n",
      "Epoch [6]\t Average validation loss 0.0679\t Average validation accuracy 0.9530\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0627\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0686\t Accuracy 0.9471\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0702\t Accuracy 0.9467\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0724\t Accuracy 0.9433\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0716\t Accuracy 0.9441\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0712\t Accuracy 0.9448\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0715\t Accuracy 0.9439\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0716\t Accuracy 0.9437\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0717\t Accuracy 0.9431\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0717\t Accuracy 0.9431\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0720\t Accuracy 0.9425\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0719\t Average training accuracy 0.9422\n",
      "Epoch [7]\t Average validation loss 0.0664\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0669\t Accuracy 0.9492\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0682\t Accuracy 0.9492\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0704\t Accuracy 0.9458\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0698\t Accuracy 0.9464\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0693\t Accuracy 0.9469\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0696\t Accuracy 0.9462\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0696\t Accuracy 0.9462\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0698\t Accuracy 0.9458\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0698\t Accuracy 0.9458\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0701\t Accuracy 0.9450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0700\t Average training accuracy 0.9449\n",
      "Epoch [8]\t Average validation loss 0.0654\t Average validation accuracy 0.9574\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0611\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0653\t Accuracy 0.9504\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0666\t Accuracy 0.9507\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0687\t Accuracy 0.9474\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0681\t Accuracy 0.9480\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0677\t Accuracy 0.9486\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0679\t Accuracy 0.9482\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0680\t Accuracy 0.9482\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0682\t Accuracy 0.9481\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0681\t Accuracy 0.9481\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0685\t Accuracy 0.9474\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0684\t Average training accuracy 0.9472\n",
      "Epoch [9]\t Average validation loss 0.0647\t Average validation accuracy 0.9574\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0604\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0640\t Accuracy 0.9525\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0651\t Accuracy 0.9525\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0672\t Accuracy 0.9491\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0667\t Accuracy 0.9495\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0663\t Accuracy 0.9498\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0665\t Accuracy 0.9492\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0665\t Accuracy 0.9493\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0667\t Accuracy 0.9491\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0667\t Accuracy 0.9490\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0670\t Accuracy 0.9483\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0670\t Average training accuracy 0.9481\n",
      "Epoch [10]\t Average validation loss 0.0637\t Average validation accuracy 0.9564\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0590\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0628\t Accuracy 0.9524\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0638\t Accuracy 0.9529\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0659\t Accuracy 0.9503\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0654\t Accuracy 0.9507\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0650\t Accuracy 0.9509\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0652\t Accuracy 0.9502\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0652\t Accuracy 0.9502\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0654\t Accuracy 0.9501\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0654\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0658\t Accuracy 0.9493\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0657\t Average training accuracy 0.9491\n",
      "Epoch [11]\t Average validation loss 0.0633\t Average validation accuracy 0.9574\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0580\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0616\t Accuracy 0.9533\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0626\t Accuracy 0.9540\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0646\t Accuracy 0.9518\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0642\t Accuracy 0.9522\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0638\t Accuracy 0.9522\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0640\t Accuracy 0.9515\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0640\t Accuracy 0.9516\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0642\t Accuracy 0.9515\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0642\t Accuracy 0.9515\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0646\t Accuracy 0.9507\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0645\t Average training accuracy 0.9505\n",
      "Epoch [12]\t Average validation loss 0.0625\t Average validation accuracy 0.9582\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0567\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0605\t Accuracy 0.9525\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0614\t Accuracy 0.9543\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0633\t Accuracy 0.9523\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0630\t Accuracy 0.9529\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0626\t Accuracy 0.9530\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0629\t Accuracy 0.9522\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0629\t Accuracy 0.9525\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0631\t Accuracy 0.9523\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0631\t Accuracy 0.9524\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0634\t Accuracy 0.9516\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0634\t Average training accuracy 0.9515\n",
      "Epoch [13]\t Average validation loss 0.0617\t Average validation accuracy 0.9594\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0555\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0595\t Accuracy 0.9531\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0604\t Accuracy 0.9552\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0623\t Accuracy 0.9536\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0619\t Accuracy 0.9542\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0615\t Accuracy 0.9542\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0619\t Accuracy 0.9535\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0619\t Accuracy 0.9536\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0621\t Accuracy 0.9535\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0620\t Accuracy 0.9536\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0624\t Accuracy 0.9528\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0623\t Average training accuracy 0.9527\n",
      "Epoch [14]\t Average validation loss 0.0610\t Average validation accuracy 0.9594\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0542\t Accuracy 0.9600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0586\t Accuracy 0.9557\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0595\t Accuracy 0.9566\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0613\t Accuracy 0.9552\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0609\t Accuracy 0.9554\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0606\t Accuracy 0.9554\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0609\t Accuracy 0.9547\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0609\t Accuracy 0.9547\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0611\t Accuracy 0.9546\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0611\t Accuracy 0.9546\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0615\t Accuracy 0.9539\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0614\t Average training accuracy 0.9538\n",
      "Epoch [15]\t Average validation loss 0.0605\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0536\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0579\t Accuracy 0.9573\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0587\t Accuracy 0.9574\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0604\t Accuracy 0.9562\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0601\t Accuracy 0.9563\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0597\t Accuracy 0.9563\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0600\t Accuracy 0.9556\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0601\t Accuracy 0.9557\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0603\t Accuracy 0.9556\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0603\t Accuracy 0.9555\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0606\t Accuracy 0.9547\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0605\t Average training accuracy 0.9548\n",
      "Epoch [16]\t Average validation loss 0.0596\t Average validation accuracy 0.9602\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0529\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0572\t Accuracy 0.9580\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0579\t Accuracy 0.9583\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0596\t Accuracy 0.9572\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0593\t Accuracy 0.9571\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0589\t Accuracy 0.9569\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0593\t Accuracy 0.9562\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0593\t Accuracy 0.9565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0595\t Accuracy 0.9564\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0595\t Accuracy 0.9563\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0598\t Accuracy 0.9556\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0598\t Average training accuracy 0.9556\n",
      "Epoch [17]\t Average validation loss 0.0591\t Average validation accuracy 0.9610\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0521\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0565\t Accuracy 0.9586\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0572\t Accuracy 0.9590\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0590\t Accuracy 0.9581\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0586\t Accuracy 0.9579\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0583\t Accuracy 0.9579\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0586\t Accuracy 0.9571\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0586\t Accuracy 0.9575\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0588\t Accuracy 0.9574\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0588\t Accuracy 0.9572\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0591\t Accuracy 0.9564\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0591\t Average training accuracy 0.9564\n",
      "Epoch [18]\t Average validation loss 0.0586\t Average validation accuracy 0.9616\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0516\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0559\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0567\t Accuracy 0.9601\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0583\t Accuracy 0.9591\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0580\t Accuracy 0.9589\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0576\t Accuracy 0.9588\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0580\t Accuracy 0.9581\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0579\t Accuracy 0.9584\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0581\t Accuracy 0.9583\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0581\t Accuracy 0.9582\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0585\t Accuracy 0.9574\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0584\t Average training accuracy 0.9573\n",
      "Epoch [19]\t Average validation loss 0.0580\t Average validation accuracy 0.9612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9533.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FCLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9ab722069b30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Build MLP with FCLayer and SigmoidLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 128 is the number of hidden units, you can change by your own\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msigmoidMLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFCLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msigmoidMLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSigmoidLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msigmoidMLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFCLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FCLayer' is not defined"
     ]
    }
   ],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5073\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.3769\t Accuracy 0.6694\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.0404\t Accuracy 0.7526\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8858\t Accuracy 0.7851\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.7845\t Accuracy 0.8076\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.7138\t Accuracy 0.8227\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.6633\t Accuracy 0.8336\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.6280\t Accuracy 0.8404\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.5968\t Accuracy 0.8468\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.5723\t Accuracy 0.8520\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.5523\t Accuracy 0.8559\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5349\t Average training accuracy 0.8594\n",
      "Epoch [0]\t Average validation loss 0.2686\t Average validation accuracy 0.9268\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.3121\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.3080\t Accuracy 0.9143\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.3174\t Accuracy 0.9106\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.3293\t Accuracy 0.9062\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.3228\t Accuracy 0.9078\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.3201\t Accuracy 0.9088\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.3185\t Accuracy 0.9093\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.3191\t Accuracy 0.9085\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3175\t Accuracy 0.9085\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3167\t Accuracy 0.9088\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3164\t Accuracy 0.9088\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3153\t Average training accuracy 0.9088\n",
      "Epoch [1]\t Average validation loss 0.2276\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.2463\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.2599\t Accuracy 0.9276\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.2711\t Accuracy 0.9250\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.2834\t Accuracy 0.9194\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.2776\t Accuracy 0.9206\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.2766\t Accuracy 0.9210\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.2760\t Accuracy 0.9213\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.2769\t Accuracy 0.9204\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.2762\t Accuracy 0.9206\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.2762\t Accuracy 0.9206\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.2766\t Accuracy 0.9203\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2761\t Average training accuracy 0.9203\n",
      "Epoch [2]\t Average validation loss 0.2033\t Average validation accuracy 0.9450\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2117\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.2298\t Accuracy 0.9363\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.2405\t Accuracy 0.9331\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.2517\t Accuracy 0.9280\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.2465\t Accuracy 0.9288\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.2462\t Accuracy 0.9292\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.2460\t Accuracy 0.9296\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.2466\t Accuracy 0.9292\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.2461\t Accuracy 0.9294\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.2464\t Accuracy 0.9294\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.2471\t Accuracy 0.9291\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2467\t Average training accuracy 0.9291\n",
      "Epoch [3]\t Average validation loss 0.1838\t Average validation accuracy 0.9520\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1873\t Accuracy 0.9600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2050\t Accuracy 0.9425\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2152\t Accuracy 0.9402\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2250\t Accuracy 0.9361\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2208\t Accuracy 0.9369\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2209\t Accuracy 0.9370\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2210\t Accuracy 0.9371\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2215\t Accuracy 0.9368\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2211\t Accuracy 0.9369\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2215\t Accuracy 0.9367\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2224\t Accuracy 0.9362\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2221\t Average training accuracy 0.9365\n",
      "Epoch [4]\t Average validation loss 0.1682\t Average validation accuracy 0.9570\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1683\t Accuracy 0.9600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1845\t Accuracy 0.9494\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1943\t Accuracy 0.9461\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2027\t Accuracy 0.9432\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1993\t Accuracy 0.9436\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1997\t Accuracy 0.9437\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2000\t Accuracy 0.9436\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2005\t Accuracy 0.9430\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2002\t Accuracy 0.9430\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2006\t Accuracy 0.9430\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2017\t Accuracy 0.9424\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2015\t Average training accuracy 0.9425\n",
      "Epoch [5]\t Average validation loss 0.1555\t Average validation accuracy 0.9596\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1527\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1674\t Accuracy 0.9553\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1767\t Accuracy 0.9507\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1839\t Accuracy 0.9484\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1812\t Accuracy 0.9492\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1818\t Accuracy 0.9491\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1823\t Accuracy 0.9489\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1828\t Accuracy 0.9483\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1826\t Accuracy 0.9483\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1830\t Accuracy 0.9482\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1842\t Accuracy 0.9475\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1840\t Average training accuracy 0.9476\n",
      "Epoch [6]\t Average validation loss 0.1451\t Average validation accuracy 0.9630\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1397\t Accuracy 0.9800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1530\t Accuracy 0.9588\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1618\t Accuracy 0.9534\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1678\t Accuracy 0.9517\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1658\t Accuracy 0.9525\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1665\t Accuracy 0.9522\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1671\t Accuracy 0.9520\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1677\t Accuracy 0.9518\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1675\t Accuracy 0.9518\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1679\t Accuracy 0.9519\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1692\t Accuracy 0.9513\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1690\t Average training accuracy 0.9513\n",
      "Epoch [7]\t Average validation loss 0.1363\t Average validation accuracy 0.9656\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1288\t Accuracy 0.9800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1407\t Accuracy 0.9622\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1490\t Accuracy 0.9566\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1540\t Accuracy 0.9560\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1526\t Accuracy 0.9564\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1532\t Accuracy 0.9560\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1540\t Accuracy 0.9556\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1546\t Accuracy 0.9554\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1545\t Accuracy 0.9554\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1548\t Accuracy 0.9556\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1562\t Accuracy 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.1559\t Average training accuracy 0.9553\n",
      "Epoch [8]\t Average validation loss 0.1287\t Average validation accuracy 0.9664\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1194\t Accuracy 0.9800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1300\t Accuracy 0.9637\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1378\t Accuracy 0.9591\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1420\t Accuracy 0.9587\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1411\t Accuracy 0.9589\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1418\t Accuracy 0.9586\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1426\t Accuracy 0.9583\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1432\t Accuracy 0.9581\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1431\t Accuracy 0.9584\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1434\t Accuracy 0.9585\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1448\t Accuracy 0.9580\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1446\t Average training accuracy 0.9581\n",
      "Epoch [9]\t Average validation loss 0.1222\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1114\t Accuracy 0.9800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.1207\t Accuracy 0.9665\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1281\t Accuracy 0.9625\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1316\t Accuracy 0.9619\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1311\t Accuracy 0.9620\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1317\t Accuracy 0.9619\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1326\t Accuracy 0.9614\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1332\t Accuracy 0.9613\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1332\t Accuracy 0.9616\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1334\t Accuracy 0.9616\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1348\t Accuracy 0.9609\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1346\t Average training accuracy 0.9610\n",
      "Epoch [10]\t Average validation loss 0.1166\t Average validation accuracy 0.9700\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1045\t Accuracy 0.9800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1126\t Accuracy 0.9682\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.1196\t Accuracy 0.9656\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1224\t Accuracy 0.9646\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1223\t Accuracy 0.9649\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1229\t Accuracy 0.9649\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1238\t Accuracy 0.9645\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1244\t Accuracy 0.9646\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1244\t Accuracy 0.9649\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1246\t Accuracy 0.9647\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1260\t Accuracy 0.9641\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1258\t Average training accuracy 0.9642\n",
      "Epoch [11]\t Average validation loss 0.1117\t Average validation accuracy 0.9714\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0985\t Accuracy 0.9800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1054\t Accuracy 0.9698\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1120\t Accuracy 0.9673\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1144\t Accuracy 0.9664\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.1145\t Accuracy 0.9666\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1151\t Accuracy 0.9666\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1160\t Accuracy 0.9666\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.1165\t Accuracy 0.9668\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1166\t Accuracy 0.9670\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1168\t Accuracy 0.9669\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1182\t Accuracy 0.9664\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1180\t Average training accuracy 0.9666\n",
      "Epoch [12]\t Average validation loss 0.1074\t Average validation accuracy 0.9726\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0933\t Accuracy 0.9800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0990\t Accuracy 0.9725\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1053\t Accuracy 0.9701\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1072\t Accuracy 0.9691\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1076\t Accuracy 0.9695\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1081\t Accuracy 0.9693\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1090\t Accuracy 0.9694\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1095\t Accuracy 0.9694\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1096\t Accuracy 0.9696\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.1098\t Accuracy 0.9694\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.1112\t Accuracy 0.9688\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1110\t Average training accuracy 0.9690\n",
      "Epoch [13]\t Average validation loss 0.1036\t Average validation accuracy 0.9732\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0886\t Accuracy 0.9800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0933\t Accuracy 0.9741\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0993\t Accuracy 0.9724\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1008\t Accuracy 0.9715\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1013\t Accuracy 0.9716\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1018\t Accuracy 0.9716\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1027\t Accuracy 0.9714\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1032\t Accuracy 0.9713\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1034\t Accuracy 0.9715\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1036\t Accuracy 0.9713\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1049\t Accuracy 0.9707\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1048\t Average training accuracy 0.9708\n",
      "Epoch [14]\t Average validation loss 0.1003\t Average validation accuracy 0.9744\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0844\t Accuracy 0.9800\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0882\t Accuracy 0.9755\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0939\t Accuracy 0.9742\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0950\t Accuracy 0.9735\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0957\t Accuracy 0.9735\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0961\t Accuracy 0.9734\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0971\t Accuracy 0.9732\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0975\t Accuracy 0.9732\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0978\t Accuracy 0.9734\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0979\t Accuracy 0.9734\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0992\t Accuracy 0.9728\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0991\t Average training accuracy 0.9728\n",
      "Epoch [15]\t Average validation loss 0.0973\t Average validation accuracy 0.9742\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0806\t Accuracy 0.9800\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0836\t Accuracy 0.9773\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0890\t Accuracy 0.9759\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0898\t Accuracy 0.9754\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0906\t Accuracy 0.9754\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0910\t Accuracy 0.9753\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0919\t Accuracy 0.9750\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0924\t Accuracy 0.9750\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0927\t Accuracy 0.9751\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0928\t Accuracy 0.9751\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0941\t Accuracy 0.9746\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0940\t Average training accuracy 0.9746\n",
      "Epoch [16]\t Average validation loss 0.0947\t Average validation accuracy 0.9748\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0772\t Accuracy 0.9800\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0793\t Accuracy 0.9778\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0846\t Accuracy 0.9773\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0851\t Accuracy 0.9768\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0859\t Accuracy 0.9768\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0863\t Accuracy 0.9765\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0872\t Accuracy 0.9761\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0877\t Accuracy 0.9762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0880\t Accuracy 0.9763\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0881\t Accuracy 0.9764\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0894\t Accuracy 0.9759\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0893\t Average training accuracy 0.9758\n",
      "Epoch [17]\t Average validation loss 0.0923\t Average validation accuracy 0.9748\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0740\t Accuracy 0.9900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0755\t Accuracy 0.9786\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0805\t Accuracy 0.9784\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0808\t Accuracy 0.9781\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0817\t Accuracy 0.9781\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0820\t Accuracy 0.9776\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0830\t Accuracy 0.9773\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0833\t Accuracy 0.9774\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0837\t Accuracy 0.9775\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0839\t Accuracy 0.9775\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0851\t Accuracy 0.9770\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0850\t Average training accuracy 0.9770\n",
      "Epoch [18]\t Average validation loss 0.0901\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0710\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0720\t Accuracy 0.9790\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0768\t Accuracy 0.9788\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0769\t Accuracy 0.9787\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0778\t Accuracy 0.9789\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0781\t Accuracy 0.9785\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0790\t Accuracy 0.9783\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0794\t Accuracy 0.9785\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0798\t Accuracy 0.9786\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0799\t Accuracy 0.9787\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0811\t Accuracy 0.9783\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0810\t Average training accuracy 0.9783\n",
      "Epoch [19]\t Average validation loss 0.0881\t Average validation accuracy 0.9768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9693.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.6733\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.6719\t Accuracy 0.8104\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.5164\t Accuracy 0.8535\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.4577\t Accuracy 0.8686\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.4113\t Accuracy 0.8816\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.3803\t Accuracy 0.8905\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.3568\t Accuracy 0.8973\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.3382\t Accuracy 0.9022\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.3226\t Accuracy 0.9068\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.3097\t Accuracy 0.9108\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.2997\t Accuracy 0.9135\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2897\t Average training accuracy 0.9163\n",
      "Epoch [0]\t Average validation loss 0.1452\t Average validation accuracy 0.9618\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1541\t Accuracy 0.9600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1424\t Accuracy 0.9614\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1523\t Accuracy 0.9571\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1573\t Accuracy 0.9555\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1522\t Accuracy 0.9572\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1494\t Accuracy 0.9578\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1478\t Accuracy 0.9578\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1457\t Accuracy 0.9584\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1439\t Accuracy 0.9590\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1424\t Accuracy 0.9594\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1425\t Accuracy 0.9593\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1407\t Average training accuracy 0.9599\n",
      "Epoch [1]\t Average validation loss 0.1091\t Average validation accuracy 0.9702\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1042\t Accuracy 0.9700\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0999\t Accuracy 0.9722\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1070\t Accuracy 0.9695\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1093\t Accuracy 0.9691\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1067\t Accuracy 0.9696\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1050\t Accuracy 0.9700\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1047\t Accuracy 0.9699\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1036\t Accuracy 0.9701\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1029\t Accuracy 0.9703\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1025\t Accuracy 0.9706\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1033\t Accuracy 0.9702\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1023\t Average training accuracy 0.9705\n",
      "Epoch [2]\t Average validation loss 0.0935\t Average validation accuracy 0.9744\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0820\t Accuracy 0.9800\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0788\t Accuracy 0.9792\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0835\t Accuracy 0.9771\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0848\t Accuracy 0.9761\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0830\t Accuracy 0.9763\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0819\t Accuracy 0.9765\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0818\t Accuracy 0.9766\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0810\t Accuracy 0.9769\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0808\t Accuracy 0.9769\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0807\t Accuracy 0.9771\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0816\t Accuracy 0.9768\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0808\t Average training accuracy 0.9771\n",
      "Epoch [3]\t Average validation loss 0.0861\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0660\t Accuracy 0.9800\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0650\t Accuracy 0.9827\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0680\t Accuracy 0.9822\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0685\t Accuracy 0.9813\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0672\t Accuracy 0.9808\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0665\t Accuracy 0.9809\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0665\t Accuracy 0.9811\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0660\t Accuracy 0.9811\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0661\t Accuracy 0.9811\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0661\t Accuracy 0.9812\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0669\t Accuracy 0.9811\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0662\t Average training accuracy 0.9813\n",
      "Epoch [4]\t Average validation loss 0.0817\t Average validation accuracy 0.9768\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0568\t Accuracy 0.9900\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0553\t Accuracy 0.9853\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0572\t Accuracy 0.9849\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0570\t Accuracy 0.9844\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0559\t Accuracy 0.9844\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0555\t Accuracy 0.9844\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0555\t Accuracy 0.9847\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0551\t Accuracy 0.9848\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0553\t Accuracy 0.9848\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0555\t Accuracy 0.9847\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0561\t Accuracy 0.9846\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0555\t Average training accuracy 0.9847\n",
      "Epoch [5]\t Average validation loss 0.0791\t Average validation accuracy 0.9782\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0492\t Accuracy 0.9900\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0480\t Accuracy 0.9875\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0491\t Accuracy 0.9869\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0481\t Accuracy 0.9870\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0471\t Accuracy 0.9869\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0467\t Accuracy 0.9870\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0468\t Accuracy 0.9873\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0465\t Accuracy 0.9874\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0468\t Accuracy 0.9874\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0470\t Accuracy 0.9874\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0475\t Accuracy 0.9874\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0470\t Average training accuracy 0.9875\n",
      "Epoch [6]\t Average validation loss 0.0779\t Average validation accuracy 0.9788\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0423\t Accuracy 0.9900\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0418\t Accuracy 0.9896\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0425\t Accuracy 0.9887\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0410\t Accuracy 0.9887\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0400\t Accuracy 0.9890\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0396\t Accuracy 0.9891\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0398\t Accuracy 0.9895\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0396\t Accuracy 0.9897\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0399\t Accuracy 0.9896\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0402\t Accuracy 0.9895\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0406\t Accuracy 0.9895\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0402\t Average training accuracy 0.9896\n",
      "Epoch [7]\t Average validation loss 0.0772\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0386\t Accuracy 0.9900\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0366\t Accuracy 0.9908\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0369\t Accuracy 0.9900\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0352\t Accuracy 0.9908\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0342\t Accuracy 0.9913\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0338\t Accuracy 0.9913\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0341\t Accuracy 0.9914\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0340\t Accuracy 0.9915\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0343\t Accuracy 0.9916\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0345\t Accuracy 0.9914\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0349\t Accuracy 0.9914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0345\t Average training accuracy 0.9915\n",
      "Epoch [8]\t Average validation loss 0.0769\t Average validation accuracy 0.9800\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0334\t Accuracy 0.9900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0323\t Accuracy 0.9929\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0322\t Accuracy 0.9925\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0304\t Accuracy 0.9928\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0294\t Accuracy 0.9931\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0290\t Accuracy 0.9929\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0293\t Accuracy 0.9929\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0293\t Accuracy 0.9931\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0296\t Accuracy 0.9931\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0298\t Accuracy 0.9929\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0301\t Accuracy 0.9930\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0297\t Average training accuracy 0.9930\n",
      "Epoch [9]\t Average validation loss 0.0770\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0276\t Accuracy 0.9900\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0283\t Accuracy 0.9947\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0281\t Accuracy 0.9942\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0263\t Accuracy 0.9944\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0254\t Accuracy 0.9946\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0250\t Accuracy 0.9947\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0253\t Accuracy 0.9948\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0253\t Accuracy 0.9948\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0256\t Accuracy 0.9948\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0257\t Accuracy 0.9947\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0260\t Accuracy 0.9946\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0256\t Average training accuracy 0.9946\n",
      "Epoch [10]\t Average validation loss 0.0769\t Average validation accuracy 0.9798\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0230\t Accuracy 0.9900\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0251\t Accuracy 0.9949\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0246\t Accuracy 0.9951\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0228\t Accuracy 0.9953\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0219\t Accuracy 0.9956\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0215\t Accuracy 0.9958\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0219\t Accuracy 0.9956\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0218\t Accuracy 0.9959\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0222\t Accuracy 0.9958\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0222\t Accuracy 0.9957\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0225\t Accuracy 0.9956\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0222\t Average training accuracy 0.9956\n",
      "Epoch [11]\t Average validation loss 0.0770\t Average validation accuracy 0.9802\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0199\t Accuracy 1.0000\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0219\t Accuracy 0.9965\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0214\t Accuracy 0.9963\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0198\t Accuracy 0.9966\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0190\t Accuracy 0.9969\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0187\t Accuracy 0.9971\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0190\t Accuracy 0.9969\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0190\t Accuracy 0.9970\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0193\t Accuracy 0.9969\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0194\t Accuracy 0.9968\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0196\t Accuracy 0.9967\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0193\t Average training accuracy 0.9967\n",
      "Epoch [12]\t Average validation loss 0.0769\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0165\t Accuracy 1.0000\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0192\t Accuracy 0.9971\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0187\t Accuracy 0.9969\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0172\t Accuracy 0.9972\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0166\t Accuracy 0.9976\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0163\t Accuracy 0.9978\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0166\t Accuracy 0.9976\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0166\t Accuracy 0.9978\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0169\t Accuracy 0.9976\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0169\t Accuracy 0.9975\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0171\t Accuracy 0.9974\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0169\t Average training accuracy 0.9974\n",
      "Epoch [13]\t Average validation loss 0.0766\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0146\t Accuracy 1.0000\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0169\t Accuracy 0.9976\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0164\t Accuracy 0.9980\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0151\t Accuracy 0.9981\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0146\t Accuracy 0.9985\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0142\t Accuracy 0.9985\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0145\t Accuracy 0.9983\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0145\t Accuracy 0.9983\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0148\t Accuracy 0.9982\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0148\t Accuracy 0.9981\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0150\t Accuracy 0.9981\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0148\t Average training accuracy 0.9981\n",
      "Epoch [14]\t Average validation loss 0.0761\t Average validation accuracy 0.9814\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0123\t Accuracy 1.0000\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0149\t Accuracy 0.9980\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0144\t Accuracy 0.9984\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0133\t Accuracy 0.9986\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0128\t Accuracy 0.9989\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0125\t Accuracy 0.9988\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0128\t Accuracy 0.9987\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0128\t Accuracy 0.9987\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0130\t Accuracy 0.9985\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0130\t Accuracy 0.9985\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0132\t Accuracy 0.9984\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0130\t Average training accuracy 0.9985\n",
      "Epoch [15]\t Average validation loss 0.0756\t Average validation accuracy 0.9818\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0114\t Accuracy 1.0000\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0131\t Accuracy 0.9982\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0126\t Accuracy 0.9987\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0117\t Accuracy 0.9989\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0113\t Accuracy 0.9991\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0110\t Accuracy 0.9992\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0113\t Accuracy 0.9991\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0113\t Accuracy 0.9991\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0115\t Accuracy 0.9989\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0115\t Accuracy 0.9989\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0117\t Accuracy 0.9988\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0115\t Average training accuracy 0.9989\n",
      "Epoch [16]\t Average validation loss 0.0752\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0101\t Accuracy 1.0000\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0116\t Accuracy 0.9982\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0113\t Accuracy 0.9989\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0105\t Accuracy 0.9991\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0102\t Accuracy 0.9994\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0099\t Accuracy 0.9994\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0101\t Accuracy 0.9994\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0101\t Accuracy 0.9993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0103\t Accuracy 0.9993\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0103\t Accuracy 0.9992\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0104\t Accuracy 0.9991\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0103\t Average training accuracy 0.9992\n",
      "Epoch [17]\t Average validation loss 0.0746\t Average validation accuracy 0.9816\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0094\t Accuracy 1.0000\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0103\t Accuracy 0.9986\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0100\t Accuracy 0.9992\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0094\t Accuracy 0.9994\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0091\t Accuracy 0.9996\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0088\t Accuracy 0.9996\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0090\t Accuracy 0.9996\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0090\t Accuracy 0.9995\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0092\t Accuracy 0.9994\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0092\t Accuracy 0.9994\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0093\t Accuracy 0.9994\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0092\t Average training accuracy 0.9994\n",
      "Epoch [18]\t Average validation loss 0.0746\t Average validation accuracy 0.9814\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0084\t Accuracy 1.0000\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0092\t Accuracy 0.9988\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0090\t Accuracy 0.9993\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0084\t Accuracy 0.9995\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0082\t Accuracy 0.9996\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0079\t Accuracy 0.9997\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0081\t Accuracy 0.9996\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0081\t Accuracy 0.9996\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0083\t Accuracy 0.9995\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0083\t Accuracy 0.9995\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0084\t Accuracy 0.9995\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0083\t Average training accuracy 0.9995\n",
      "Epoch [19]\t Average validation loss 0.0742\t Average validation accuracy 0.9818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9788.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework1-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**\n",
    "\n",
    "Build and train a MLP containing two hidden layer with 256,128 units respectively using ReLU activation function and Softmax cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiMLP = Network()\n",
    "multiMLP.add(FCLayer(784, 256))\n",
    "multiMLP.add(ReLULayer())\n",
    "multiMLP.add(FCLayer(256, 128))\n",
    "multiMLP.add(ReLULayer())\n",
    "multiMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5ac76e2b9fb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmultiMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultiMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisp_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "multiMLP, multi_loss, multi_acc = train(multiMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(multiMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_acc({'one_hidden_layer': [relu_loss, relu_acc],\n",
    "                   'two_hidden_layer': [multi_loss, multi_acc]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
