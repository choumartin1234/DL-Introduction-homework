{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-1: Softmax for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- ### implement and apply a softmax classifier to perform digits classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "The **mnist_data_loader.py** is a script to load mnist dataset, which is included in [TensorFlow tutorial](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Training dataset size:  60000\n",
      "Test dataset size:  10000\n"
     ]
    }
   ],
   "source": [
    "import mnist_data_loader\n",
    "\n",
    "mnist_dataset = mnist_data_loader.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "# training dataset\n",
    "train_set = mnist_dataset.train \n",
    "# test dataset\n",
    "test_set = mnist_dataset.test   \n",
    "\n",
    "train_size = train_set.num_examples\n",
    "test_size = test_set.num_examples\n",
    "print()\n",
    "print('Training dataset size: ', train_size)\n",
    "print('Test dataset size: ', test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Example\n",
    "To have a better understand of MNIST dataset, we can visualize some examples in MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2e300006cc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can choose any image in training set to visualize, example_id ∈ [0,60000)\n",
    "example_id = 0 \n",
    "\n",
    "image = train_set.images[example_id] # shape = 784 (28*28)\n",
    "label = train_set.labels[example_id] # shape = 10\n",
    "#print(image)\n",
    "print(label)\n",
    "plt.imshow(np.reshape(image,[28,28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "learning_rate = 0.05\n",
    "\n",
    "# For regularization\n",
    "lamda = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Before executing the following code, you should accomplish **./softmax_classifier.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from softmax_classifier import softmax_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][200]\t Batch [0][6000]\t Training Loss -2.3001\t Accuracy 0.0000\n",
      "Epoch [0][200]\t Batch [100][6000]\t Training Loss -1.1090\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [200][6000]\t Training Loss -0.7645\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [300][6000]\t Training Loss -1.0335\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [400][6000]\t Training Loss -0.8865\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [500][6000]\t Training Loss -0.7536\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [600][6000]\t Training Loss -0.7361\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [700][6000]\t Training Loss -0.8505\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [800][6000]\t Training Loss -0.6421\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [900][6000]\t Training Loss -0.9672\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [1000][6000]\t Training Loss -0.6615\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [1100][6000]\t Training Loss -0.5719\t Accuracy 1.0000\n",
      "Epoch [0][200]\t Batch [1200][6000]\t Training Loss -0.7316\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [1300][6000]\t Training Loss -1.0567\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [1400][6000]\t Training Loss -1.1844\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [1500][6000]\t Training Loss -0.8985\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [1600][6000]\t Training Loss -1.1108\t Accuracy 0.4000\n",
      "Epoch [0][200]\t Batch [1700][6000]\t Training Loss -0.9268\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [1800][6000]\t Training Loss -1.3865\t Accuracy 0.5000\n",
      "Epoch [0][200]\t Batch [1900][6000]\t Training Loss -1.1644\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [2000][6000]\t Training Loss -0.8783\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [2100][6000]\t Training Loss -0.9239\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [2200][6000]\t Training Loss -0.6690\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [2300][6000]\t Training Loss -1.0379\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [2400][6000]\t Training Loss -1.1404\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [2500][6000]\t Training Loss -0.9130\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [2600][6000]\t Training Loss -0.7111\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [2700][6000]\t Training Loss -1.0577\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [2800][6000]\t Training Loss -0.6731\t Accuracy 1.0000\n",
      "Epoch [0][200]\t Batch [2900][6000]\t Training Loss -1.2637\t Accuracy 0.5000\n",
      "Epoch [0][200]\t Batch [3000][6000]\t Training Loss -0.8215\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [3100][6000]\t Training Loss -0.9774\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [3200][6000]\t Training Loss -0.6896\t Accuracy 1.0000\n",
      "Epoch [0][200]\t Batch [3300][6000]\t Training Loss -0.9189\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [3400][6000]\t Training Loss -0.9385\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [3500][6000]\t Training Loss -0.8338\t Accuracy 1.0000\n",
      "Epoch [0][200]\t Batch [3600][6000]\t Training Loss -1.1645\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [3700][6000]\t Training Loss -0.7283\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [3800][6000]\t Training Loss -0.9385\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [3900][6000]\t Training Loss -0.9873\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [4000][6000]\t Training Loss -0.9086\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [4100][6000]\t Training Loss -1.0819\t Accuracy 0.5000\n",
      "Epoch [0][200]\t Batch [4200][6000]\t Training Loss -0.9293\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [4300][6000]\t Training Loss -1.0122\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [4400][6000]\t Training Loss -1.1059\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [4500][6000]\t Training Loss -1.1426\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [4600][6000]\t Training Loss -1.1885\t Accuracy 0.5000\n",
      "Epoch [0][200]\t Batch [4700][6000]\t Training Loss -0.8007\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [4800][6000]\t Training Loss -1.0942\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [4900][6000]\t Training Loss -1.0667\t Accuracy 1.0000\n",
      "Epoch [0][200]\t Batch [5000][6000]\t Training Loss -1.1130\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [5100][6000]\t Training Loss -0.8333\t Accuracy 1.0000\n",
      "Epoch [0][200]\t Batch [5200][6000]\t Training Loss -0.8675\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [5300][6000]\t Training Loss -0.7537\t Accuracy 1.0000\n",
      "Epoch [0][200]\t Batch [5400][6000]\t Training Loss -1.1186\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [5500][6000]\t Training Loss -0.8980\t Accuracy 0.9000\n",
      "Epoch [0][200]\t Batch [5600][6000]\t Training Loss -0.8080\t Accuracy 0.6000\n",
      "Epoch [0][200]\t Batch [5700][6000]\t Training Loss -0.9378\t Accuracy 0.8000\n",
      "Epoch [0][200]\t Batch [5800][6000]\t Training Loss -0.9723\t Accuracy 0.7000\n",
      "Epoch [0][200]\t Batch [5900][6000]\t Training Loss -0.7515\t Accuracy 0.9000\n",
      "\n",
      "Epoch [1][200]\t Batch [0][6000]\t Training Loss -0.8879\t Accuracy 0.6000\n",
      "Epoch [1][200]\t Batch [100][6000]\t Training Loss -0.8874\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [200][6000]\t Training Loss -0.9838\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [300][6000]\t Training Loss -0.9014\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [400][6000]\t Training Loss -0.7870\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [500][6000]\t Training Loss -0.9826\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [600][6000]\t Training Loss -0.9022\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [700][6000]\t Training Loss -1.1537\t Accuracy 0.5000\n",
      "Epoch [1][200]\t Batch [800][6000]\t Training Loss -0.9786\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [900][6000]\t Training Loss -0.8906\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [1000][6000]\t Training Loss -1.1381\t Accuracy 0.6000\n",
      "Epoch [1][200]\t Batch [1100][6000]\t Training Loss -0.8429\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [1200][6000]\t Training Loss -0.7721\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [1300][6000]\t Training Loss -0.9145\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [1400][6000]\t Training Loss -0.9814\t Accuracy 0.9000\n",
      "Epoch [1][200]\t Batch [1500][6000]\t Training Loss -0.7379\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [1600][6000]\t Training Loss -0.8960\t Accuracy 0.9000\n",
      "Epoch [1][200]\t Batch [1700][6000]\t Training Loss -0.8925\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [1800][6000]\t Training Loss -1.1023\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [1900][6000]\t Training Loss -1.0585\t Accuracy 0.5000\n",
      "Epoch [1][200]\t Batch [2000][6000]\t Training Loss -1.2029\t Accuracy 0.4000\n",
      "Epoch [1][200]\t Batch [2100][6000]\t Training Loss -1.1693\t Accuracy 0.4000\n",
      "Epoch [1][200]\t Batch [2200][6000]\t Training Loss -0.8082\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [2300][6000]\t Training Loss -1.1782\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [2400][6000]\t Training Loss -1.2776\t Accuracy 0.5000\n",
      "Epoch [1][200]\t Batch [2500][6000]\t Training Loss -0.9894\t Accuracy 0.6000\n",
      "Epoch [1][200]\t Batch [2600][6000]\t Training Loss -0.8008\t Accuracy 0.9000\n",
      "Epoch [1][200]\t Batch [2700][6000]\t Training Loss -0.9927\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [2800][6000]\t Training Loss -0.9281\t Accuracy 0.9000\n",
      "Epoch [1][200]\t Batch [2900][6000]\t Training Loss -0.6617\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [3000][6000]\t Training Loss -0.8355\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [3100][6000]\t Training Loss -1.1815\t Accuracy 0.5000\n",
      "Epoch [1][200]\t Batch [3200][6000]\t Training Loss -0.8820\t Accuracy 0.9000\n",
      "Epoch [1][200]\t Batch [3300][6000]\t Training Loss -0.8467\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [3400][6000]\t Training Loss -1.0351\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [3500][6000]\t Training Loss -1.0338\t Accuracy 0.9000\n",
      "Epoch [1][200]\t Batch [3600][6000]\t Training Loss -0.9160\t Accuracy 0.9000\n",
      "Epoch [1][200]\t Batch [3700][6000]\t Training Loss -0.9289\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [3800][6000]\t Training Loss -0.9876\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [3900][6000]\t Training Loss -0.9234\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [4000][6000]\t Training Loss -0.8276\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [4100][6000]\t Training Loss -0.8553\t Accuracy 0.6000\n",
      "Epoch [1][200]\t Batch [4200][6000]\t Training Loss -0.9651\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [4300][6000]\t Training Loss -1.1337\t Accuracy 0.5000\n",
      "Epoch [1][200]\t Batch [4400][6000]\t Training Loss -1.2158\t Accuracy 0.6000\n",
      "Epoch [1][200]\t Batch [4500][6000]\t Training Loss -0.8657\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [4600][6000]\t Training Loss -1.1811\t Accuracy 0.5000\n",
      "Epoch [1][200]\t Batch [4700][6000]\t Training Loss -0.8886\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [4800][6000]\t Training Loss -0.7620\t Accuracy 1.0000\n",
      "Epoch [1][200]\t Batch [4900][6000]\t Training Loss -1.1818\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [5000][6000]\t Training Loss -0.8589\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [5100][6000]\t Training Loss -0.8362\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [5200][6000]\t Training Loss -1.2106\t Accuracy 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1][200]\t Batch [5300][6000]\t Training Loss -0.9074\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [5400][6000]\t Training Loss -0.8575\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [5500][6000]\t Training Loss -1.0353\t Accuracy 0.8000\n",
      "Epoch [1][200]\t Batch [5600][6000]\t Training Loss -1.1617\t Accuracy 0.5000\n",
      "Epoch [1][200]\t Batch [5700][6000]\t Training Loss -0.9733\t Accuracy 0.9000\n",
      "Epoch [1][200]\t Batch [5800][6000]\t Training Loss -0.9407\t Accuracy 0.7000\n",
      "Epoch [1][200]\t Batch [5900][6000]\t Training Loss -1.2928\t Accuracy 0.4000\n",
      "\n",
      "Epoch [2][200]\t Batch [0][6000]\t Training Loss -1.0335\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [100][6000]\t Training Loss -0.9956\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [200][6000]\t Training Loss -0.8282\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [300][6000]\t Training Loss -0.7933\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [400][6000]\t Training Loss -1.0019\t Accuracy 0.5000\n",
      "Epoch [2][200]\t Batch [500][6000]\t Training Loss -1.0325\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [600][6000]\t Training Loss -0.9843\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [700][6000]\t Training Loss -0.8184\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [800][6000]\t Training Loss -0.9584\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [900][6000]\t Training Loss -0.9791\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [1000][6000]\t Training Loss -0.8954\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [1100][6000]\t Training Loss -0.8692\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [1200][6000]\t Training Loss -0.8408\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [1300][6000]\t Training Loss -1.2363\t Accuracy 0.5000\n",
      "Epoch [2][200]\t Batch [1400][6000]\t Training Loss -0.9007\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [1500][6000]\t Training Loss -0.9385\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [1600][6000]\t Training Loss -0.6126\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [1700][6000]\t Training Loss -1.0248\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [1800][6000]\t Training Loss -0.8447\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [1900][6000]\t Training Loss -1.3085\t Accuracy 0.4000\n",
      "Epoch [2][200]\t Batch [2000][6000]\t Training Loss -1.0210\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [2100][6000]\t Training Loss -0.7741\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [2200][6000]\t Training Loss -0.6527\t Accuracy 1.0000\n",
      "Epoch [2][200]\t Batch [2300][6000]\t Training Loss -1.0667\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [2400][6000]\t Training Loss -0.8356\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [2500][6000]\t Training Loss -0.9893\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [2600][6000]\t Training Loss -1.0497\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [2700][6000]\t Training Loss -0.9779\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [2800][6000]\t Training Loss -0.9644\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [2900][6000]\t Training Loss -1.0384\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [3000][6000]\t Training Loss -0.8699\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [3100][6000]\t Training Loss -0.8679\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [3200][6000]\t Training Loss -1.0616\t Accuracy 0.5000\n",
      "Epoch [2][200]\t Batch [3300][6000]\t Training Loss -0.9874\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [3400][6000]\t Training Loss -0.7865\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [3500][6000]\t Training Loss -1.0600\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [3600][6000]\t Training Loss -0.8505\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [3700][6000]\t Training Loss -0.8779\t Accuracy 1.0000\n",
      "Epoch [2][200]\t Batch [3800][6000]\t Training Loss -0.6919\t Accuracy 0.9000\n",
      "Epoch [2][200]\t Batch [3900][6000]\t Training Loss -0.9876\t Accuracy 0.6000\n",
      "Epoch [2][200]\t Batch [4000][6000]\t Training Loss -0.9583\t Accuracy 0.6000\n",
      "Epoch [2][200]\t Batch [4100][6000]\t Training Loss -0.6658\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [4200][6000]\t Training Loss -1.0335\t Accuracy 0.6000\n",
      "Epoch [2][200]\t Batch [4300][6000]\t Training Loss -1.0359\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [4400][6000]\t Training Loss -0.9315\t Accuracy 0.6000\n",
      "Epoch [2][200]\t Batch [4500][6000]\t Training Loss -1.1757\t Accuracy 0.6000\n",
      "Epoch [2][200]\t Batch [4600][6000]\t Training Loss -1.0654\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [4700][6000]\t Training Loss -1.3420\t Accuracy 0.4000\n",
      "Epoch [2][200]\t Batch [4800][6000]\t Training Loss -1.0822\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [4900][6000]\t Training Loss -0.8516\t Accuracy 1.0000\n",
      "Epoch [2][200]\t Batch [5000][6000]\t Training Loss -1.0743\t Accuracy 0.5000\n",
      "Epoch [2][200]\t Batch [5100][6000]\t Training Loss -1.2543\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [5200][6000]\t Training Loss -0.9266\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [5300][6000]\t Training Loss -1.0614\t Accuracy 0.7000\n",
      "Epoch [2][200]\t Batch [5400][6000]\t Training Loss -1.1031\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [5500][6000]\t Training Loss -0.9791\t Accuracy 0.6000\n",
      "Epoch [2][200]\t Batch [5600][6000]\t Training Loss -1.1883\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [5700][6000]\t Training Loss -1.3926\t Accuracy 0.4000\n",
      "Epoch [2][200]\t Batch [5800][6000]\t Training Loss -0.9654\t Accuracy 0.8000\n",
      "Epoch [2][200]\t Batch [5900][6000]\t Training Loss -1.2392\t Accuracy 0.3000\n",
      "\n",
      "Epoch [3][200]\t Batch [0][6000]\t Training Loss -1.3630\t Accuracy 0.3000\n",
      "Epoch [3][200]\t Batch [100][6000]\t Training Loss -1.0409\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [200][6000]\t Training Loss -1.0319\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [300][6000]\t Training Loss -0.9166\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [400][6000]\t Training Loss -1.1466\t Accuracy 0.5000\n",
      "Epoch [3][200]\t Batch [500][6000]\t Training Loss -0.6539\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [600][6000]\t Training Loss -0.8371\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [700][6000]\t Training Loss -1.4405\t Accuracy 0.5000\n",
      "Epoch [3][200]\t Batch [800][6000]\t Training Loss -1.0148\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [900][6000]\t Training Loss -1.1526\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [1000][6000]\t Training Loss -1.1508\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [1100][6000]\t Training Loss -1.0741\t Accuracy 0.5000\n",
      "Epoch [3][200]\t Batch [1200][6000]\t Training Loss -1.1531\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [1300][6000]\t Training Loss -1.1562\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [1400][6000]\t Training Loss -0.9556\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [1500][6000]\t Training Loss -1.1072\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [1600][6000]\t Training Loss -1.1762\t Accuracy 0.3000\n",
      "Epoch [3][200]\t Batch [1700][6000]\t Training Loss -1.0593\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [1800][6000]\t Training Loss -0.9351\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [1900][6000]\t Training Loss -0.8772\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [2000][6000]\t Training Loss -0.7212\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [2100][6000]\t Training Loss -0.9538\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [2200][6000]\t Training Loss -1.2409\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [2300][6000]\t Training Loss -1.0307\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [2400][6000]\t Training Loss -1.0369\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [2500][6000]\t Training Loss -0.9094\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [2600][6000]\t Training Loss -0.8243\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [2700][6000]\t Training Loss -0.8339\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [2800][6000]\t Training Loss -1.2657\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [2900][6000]\t Training Loss -1.0350\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [3000][6000]\t Training Loss -1.2627\t Accuracy 0.3000\n",
      "Epoch [3][200]\t Batch [3100][6000]\t Training Loss -0.7750\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [3200][6000]\t Training Loss -1.0453\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [3300][6000]\t Training Loss -1.2185\t Accuracy 0.4000\n",
      "Epoch [3][200]\t Batch [3400][6000]\t Training Loss -0.8108\t Accuracy 1.0000\n",
      "Epoch [3][200]\t Batch [3500][6000]\t Training Loss -1.0726\t Accuracy 0.5000\n",
      "Epoch [3][200]\t Batch [3600][6000]\t Training Loss -1.2159\t Accuracy 0.5000\n",
      "Epoch [3][200]\t Batch [3700][6000]\t Training Loss -0.5753\t Accuracy 1.0000\n",
      "Epoch [3][200]\t Batch [3800][6000]\t Training Loss -1.3247\t Accuracy 0.4000\n",
      "Epoch [3][200]\t Batch [3900][6000]\t Training Loss -0.8520\t Accuracy 1.0000\n",
      "Epoch [3][200]\t Batch [4000][6000]\t Training Loss -1.0159\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [4100][6000]\t Training Loss -0.9581\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [4200][6000]\t Training Loss -1.1325\t Accuracy 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3][200]\t Batch [4300][6000]\t Training Loss -1.1566\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [4400][6000]\t Training Loss -0.7709\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [4500][6000]\t Training Loss -1.3026\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [4600][6000]\t Training Loss -0.6057\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [4700][6000]\t Training Loss -0.9357\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [4800][6000]\t Training Loss -0.8358\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [4900][6000]\t Training Loss -0.8332\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [5000][6000]\t Training Loss -0.7960\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [5100][6000]\t Training Loss -1.0568\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [5200][6000]\t Training Loss -1.1281\t Accuracy 0.7000\n",
      "Epoch [3][200]\t Batch [5300][6000]\t Training Loss -1.0066\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [5400][6000]\t Training Loss -0.7515\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [5500][6000]\t Training Loss -0.8286\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [5600][6000]\t Training Loss -0.8960\t Accuracy 0.9000\n",
      "Epoch [3][200]\t Batch [5700][6000]\t Training Loss -1.0180\t Accuracy 0.6000\n",
      "Epoch [3][200]\t Batch [5800][6000]\t Training Loss -0.9765\t Accuracy 0.8000\n",
      "Epoch [3][200]\t Batch [5900][6000]\t Training Loss -1.1813\t Accuracy 0.7000\n",
      "\n",
      "Epoch [4][200]\t Batch [0][6000]\t Training Loss -0.7854\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [100][6000]\t Training Loss -0.9980\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [200][6000]\t Training Loss -0.7388\t Accuracy 0.9000\n",
      "Epoch [4][200]\t Batch [300][6000]\t Training Loss -1.0161\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [400][6000]\t Training Loss -0.9922\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [500][6000]\t Training Loss -1.3129\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [600][6000]\t Training Loss -0.8875\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [700][6000]\t Training Loss -1.0801\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [800][6000]\t Training Loss -0.8634\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [900][6000]\t Training Loss -0.8534\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [1000][6000]\t Training Loss -1.0486\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [1100][6000]\t Training Loss -0.9214\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [1200][6000]\t Training Loss -0.8093\t Accuracy 1.0000\n",
      "Epoch [4][200]\t Batch [1300][6000]\t Training Loss -0.9322\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [1400][6000]\t Training Loss -0.8499\t Accuracy 0.9000\n",
      "Epoch [4][200]\t Batch [1500][6000]\t Training Loss -1.1130\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [1600][6000]\t Training Loss -1.0236\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [1700][6000]\t Training Loss -0.9448\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [1800][6000]\t Training Loss -0.7437\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [1900][6000]\t Training Loss -0.8521\t Accuracy 0.9000\n",
      "Epoch [4][200]\t Batch [2000][6000]\t Training Loss -1.2309\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [2100][6000]\t Training Loss -1.1299\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [2200][6000]\t Training Loss -1.2357\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [2300][6000]\t Training Loss -1.0310\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [2400][6000]\t Training Loss -0.9400\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [2500][6000]\t Training Loss -0.9368\t Accuracy 0.9000\n",
      "Epoch [4][200]\t Batch [2600][6000]\t Training Loss -0.9783\t Accuracy 0.5000\n",
      "Epoch [4][200]\t Batch [2700][6000]\t Training Loss -1.1892\t Accuracy 0.5000\n",
      "Epoch [4][200]\t Batch [2800][6000]\t Training Loss -1.1249\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [2900][6000]\t Training Loss -0.7530\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [3000][6000]\t Training Loss -1.1021\t Accuracy 0.4000\n",
      "Epoch [4][200]\t Batch [3100][6000]\t Training Loss -1.0511\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [3200][6000]\t Training Loss -1.0657\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [3300][6000]\t Training Loss -0.9269\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [3400][6000]\t Training Loss -0.8644\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [3500][6000]\t Training Loss -0.6573\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [3600][6000]\t Training Loss -0.8907\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [3700][6000]\t Training Loss -0.8769\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [3800][6000]\t Training Loss -1.1088\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [3900][6000]\t Training Loss -0.8134\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [4000][6000]\t Training Loss -1.0518\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [4100][6000]\t Training Loss -1.1001\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [4200][6000]\t Training Loss -0.8287\t Accuracy 0.9000\n",
      "Epoch [4][200]\t Batch [4300][6000]\t Training Loss -0.7688\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [4400][6000]\t Training Loss -0.9252\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [4500][6000]\t Training Loss -0.8084\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [4600][6000]\t Training Loss -1.0832\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [4700][6000]\t Training Loss -1.1016\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [4800][6000]\t Training Loss -0.8935\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [4900][6000]\t Training Loss -0.9007\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [5000][6000]\t Training Loss -0.6978\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [5100][6000]\t Training Loss -0.9438\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [5200][6000]\t Training Loss -1.2764\t Accuracy 0.5000\n",
      "Epoch [4][200]\t Batch [5300][6000]\t Training Loss -0.9087\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [5400][6000]\t Training Loss -1.0552\t Accuracy 0.6000\n",
      "Epoch [4][200]\t Batch [5500][6000]\t Training Loss -0.8745\t Accuracy 0.8000\n",
      "Epoch [4][200]\t Batch [5600][6000]\t Training Loss -1.2828\t Accuracy 0.5000\n",
      "Epoch [4][200]\t Batch [5700][6000]\t Training Loss -0.8689\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [5800][6000]\t Training Loss -0.9911\t Accuracy 0.7000\n",
      "Epoch [4][200]\t Batch [5900][6000]\t Training Loss -1.1144\t Accuracy 0.5000\n",
      "\n",
      "Epoch [5][200]\t Batch [0][6000]\t Training Loss -0.8844\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [100][6000]\t Training Loss -0.9524\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [200][6000]\t Training Loss -0.9868\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [300][6000]\t Training Loss -0.9316\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [400][6000]\t Training Loss -0.6732\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [500][6000]\t Training Loss -1.0682\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [600][6000]\t Training Loss -1.1195\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [700][6000]\t Training Loss -1.1194\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [800][6000]\t Training Loss -1.0276\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [900][6000]\t Training Loss -0.9315\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [1000][6000]\t Training Loss -0.9128\t Accuracy 0.6000\n",
      "Epoch [5][200]\t Batch [1100][6000]\t Training Loss -0.9346\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [1200][6000]\t Training Loss -0.9358\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [1300][6000]\t Training Loss -1.1654\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [1400][6000]\t Training Loss -1.0411\t Accuracy 0.5000\n",
      "Epoch [5][200]\t Batch [1500][6000]\t Training Loss -0.8525\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [1600][6000]\t Training Loss -1.0300\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [1700][6000]\t Training Loss -0.9434\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [1800][6000]\t Training Loss -0.9071\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [1900][6000]\t Training Loss -1.1223\t Accuracy 0.6000\n",
      "Epoch [5][200]\t Batch [2000][6000]\t Training Loss -0.7230\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [2100][6000]\t Training Loss -0.9764\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [2200][6000]\t Training Loss -1.3022\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [2300][6000]\t Training Loss -0.7672\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [2400][6000]\t Training Loss -0.9586\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [2500][6000]\t Training Loss -0.9682\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [2600][6000]\t Training Loss -1.1799\t Accuracy 0.6000\n",
      "Epoch [5][200]\t Batch [2700][6000]\t Training Loss -0.9214\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [2800][6000]\t Training Loss -0.9194\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [2900][6000]\t Training Loss -1.1730\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [3000][6000]\t Training Loss -1.0561\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [3100][6000]\t Training Loss -0.8746\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [3200][6000]\t Training Loss -1.1494\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [3300][6000]\t Training Loss -1.1986\t Accuracy 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5][200]\t Batch [3400][6000]\t Training Loss -0.7466\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [3500][6000]\t Training Loss -1.2912\t Accuracy 0.5000\n",
      "Epoch [5][200]\t Batch [3600][6000]\t Training Loss -0.9990\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [3700][6000]\t Training Loss -1.3330\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [3800][6000]\t Training Loss -0.9372\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [3900][6000]\t Training Loss -0.8158\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [4000][6000]\t Training Loss -1.3709\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [4100][6000]\t Training Loss -0.9529\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [4200][6000]\t Training Loss -1.0491\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [4300][6000]\t Training Loss -1.0425\t Accuracy 0.6000\n",
      "Epoch [5][200]\t Batch [4400][6000]\t Training Loss -0.8989\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [4500][6000]\t Training Loss -1.0755\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [4600][6000]\t Training Loss -1.0544\t Accuracy 0.6000\n",
      "Epoch [5][200]\t Batch [4700][6000]\t Training Loss -0.9598\t Accuracy 1.0000\n",
      "Epoch [5][200]\t Batch [4800][6000]\t Training Loss -0.6910\t Accuracy 1.0000\n",
      "Epoch [5][200]\t Batch [4900][6000]\t Training Loss -0.6865\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [5000][6000]\t Training Loss -0.8178\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [5100][6000]\t Training Loss -1.2610\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [5200][6000]\t Training Loss -1.0740\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [5300][6000]\t Training Loss -0.9571\t Accuracy 0.5000\n",
      "Epoch [5][200]\t Batch [5400][6000]\t Training Loss -0.7809\t Accuracy 0.8000\n",
      "Epoch [5][200]\t Batch [5500][6000]\t Training Loss -0.9913\t Accuracy 0.9000\n",
      "Epoch [5][200]\t Batch [5600][6000]\t Training Loss -0.8442\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [5700][6000]\t Training Loss -1.0319\t Accuracy 0.7000\n",
      "Epoch [5][200]\t Batch [5800][6000]\t Training Loss -1.1582\t Accuracy 0.5000\n",
      "Epoch [5][200]\t Batch [5900][6000]\t Training Loss -0.6822\t Accuracy 0.9000\n",
      "\n",
      "Epoch [6][200]\t Batch [0][6000]\t Training Loss -0.9237\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [100][6000]\t Training Loss -1.2172\t Accuracy 0.5000\n",
      "Epoch [6][200]\t Batch [200][6000]\t Training Loss -1.0471\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [300][6000]\t Training Loss -1.1844\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [400][6000]\t Training Loss -1.2772\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [500][6000]\t Training Loss -0.9531\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [600][6000]\t Training Loss -0.9557\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [700][6000]\t Training Loss -0.9518\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [800][6000]\t Training Loss -1.1733\t Accuracy 0.4000\n",
      "Epoch [6][200]\t Batch [900][6000]\t Training Loss -1.2558\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [1000][6000]\t Training Loss -1.2463\t Accuracy 0.5000\n",
      "Epoch [6][200]\t Batch [1100][6000]\t Training Loss -0.7748\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [1200][6000]\t Training Loss -0.9313\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [1300][6000]\t Training Loss -0.6834\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [1400][6000]\t Training Loss -0.9324\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [1500][6000]\t Training Loss -1.0027\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [1600][6000]\t Training Loss -0.9498\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [1700][6000]\t Training Loss -0.7895\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [1800][6000]\t Training Loss -1.0182\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [1900][6000]\t Training Loss -0.4861\t Accuracy 1.0000\n",
      "Epoch [6][200]\t Batch [2000][6000]\t Training Loss -1.0243\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [2100][6000]\t Training Loss -0.7742\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [2200][6000]\t Training Loss -0.9514\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [2300][6000]\t Training Loss -1.1498\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [2400][6000]\t Training Loss -1.1141\t Accuracy 0.5000\n",
      "Epoch [6][200]\t Batch [2500][6000]\t Training Loss -0.7252\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [2600][6000]\t Training Loss -0.7984\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [2700][6000]\t Training Loss -0.8468\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [2800][6000]\t Training Loss -0.7817\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [2900][6000]\t Training Loss -1.0895\t Accuracy 0.5000\n",
      "Epoch [6][200]\t Batch [3000][6000]\t Training Loss -0.8699\t Accuracy 1.0000\n",
      "Epoch [6][200]\t Batch [3100][6000]\t Training Loss -0.9724\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [3200][6000]\t Training Loss -0.7686\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [3300][6000]\t Training Loss -0.9535\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [3400][6000]\t Training Loss -0.9906\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [3500][6000]\t Training Loss -0.9211\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [3600][6000]\t Training Loss -1.1946\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [3700][6000]\t Training Loss -1.1020\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [3800][6000]\t Training Loss -1.3664\t Accuracy 0.3000\n",
      "Epoch [6][200]\t Batch [3900][6000]\t Training Loss -1.0139\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [4000][6000]\t Training Loss -0.8713\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [4100][6000]\t Training Loss -0.9233\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [4200][6000]\t Training Loss -0.8948\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [4300][6000]\t Training Loss -1.1698\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [4400][6000]\t Training Loss -0.9810\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [4500][6000]\t Training Loss -1.1008\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [4600][6000]\t Training Loss -1.0561\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [4700][6000]\t Training Loss -1.3257\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [4800][6000]\t Training Loss -1.4033\t Accuracy 0.4000\n",
      "Epoch [6][200]\t Batch [4900][6000]\t Training Loss -0.9988\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [5000][6000]\t Training Loss -1.1112\t Accuracy 0.5000\n",
      "Epoch [6][200]\t Batch [5100][6000]\t Training Loss -0.9566\t Accuracy 0.8000\n",
      "Epoch [6][200]\t Batch [5200][6000]\t Training Loss -1.1872\t Accuracy 0.5000\n",
      "Epoch [6][200]\t Batch [5300][6000]\t Training Loss -0.6673\t Accuracy 0.9000\n",
      "Epoch [6][200]\t Batch [5400][6000]\t Training Loss -0.9247\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [5500][6000]\t Training Loss -1.1434\t Accuracy 0.4000\n",
      "Epoch [6][200]\t Batch [5600][6000]\t Training Loss -0.9599\t Accuracy 0.7000\n",
      "Epoch [6][200]\t Batch [5700][6000]\t Training Loss -1.2527\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [5800][6000]\t Training Loss -1.0532\t Accuracy 0.6000\n",
      "Epoch [6][200]\t Batch [5900][6000]\t Training Loss -1.1891\t Accuracy 0.5000\n",
      "\n",
      "Epoch [7][200]\t Batch [0][6000]\t Training Loss -0.7320\t Accuracy 0.9000\n",
      "Epoch [7][200]\t Batch [100][6000]\t Training Loss -0.8536\t Accuracy 0.7000\n",
      "Epoch [7][200]\t Batch [200][6000]\t Training Loss -0.9962\t Accuracy 0.9000\n",
      "Epoch [7][200]\t Batch [300][6000]\t Training Loss -1.0954\t Accuracy 0.8000\n",
      "Epoch [7][200]\t Batch [400][6000]\t Training Loss -1.0433\t Accuracy 0.6000\n",
      "Epoch [7][200]\t Batch [500][6000]\t Training Loss -1.0054\t Accuracy 0.6000\n",
      "Epoch [7][200]\t Batch [600][6000]\t Training Loss -1.1276\t Accuracy 0.6000\n",
      "Epoch [7][200]\t Batch [700][6000]\t Training Loss -0.9763\t Accuracy 0.9000\n",
      "Epoch [7][200]\t Batch [800][6000]\t Training Loss -1.2243\t Accuracy 0.3000\n",
      "Epoch [7][200]\t Batch [900][6000]\t Training Loss -0.9232\t Accuracy 0.7000\n",
      "Epoch [7][200]\t Batch [1000][6000]\t Training Loss -1.0311\t Accuracy 0.6000\n",
      "Epoch [7][200]\t Batch [1100][6000]\t Training Loss -0.9249\t Accuracy 0.7000\n",
      "Epoch [7][200]\t Batch [1200][6000]\t Training Loss -0.9620\t Accuracy 0.8000\n",
      "Epoch [7][200]\t Batch [1300][6000]\t Training Loss -0.8611\t Accuracy 1.0000\n",
      "Epoch [7][200]\t Batch [1400][6000]\t Training Loss -1.0812\t Accuracy 0.6000\n",
      "Epoch [7][200]\t Batch [1500][6000]\t Training Loss -0.9151\t Accuracy 0.8000\n",
      "Epoch [7][200]\t Batch [1600][6000]\t Training Loss -1.0968\t Accuracy 0.9000\n",
      "Epoch [7][200]\t Batch [1700][6000]\t Training Loss -0.7623\t Accuracy 0.9000\n",
      "Epoch [7][200]\t Batch [1800][6000]\t Training Loss -1.0025\t Accuracy 0.8000\n",
      "Epoch [7][200]\t Batch [1900][6000]\t Training Loss -0.8646\t Accuracy 0.7000\n",
      "Epoch [7][200]\t Batch [2000][6000]\t Training Loss -1.2281\t Accuracy 0.7000\n",
      "Epoch [7][200]\t Batch [2100][6000]\t Training Loss -1.1696\t Accuracy 0.6000\n",
      "Epoch [7][200]\t Batch [2200][6000]\t Training Loss -0.8646\t Accuracy 0.8000\n",
      "Epoch [7][200]\t Batch [2300][6000]\t Training Loss -1.4439\t Accuracy 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Weight Initialization\n",
    "W = np.random.randn(28*28, 10) * 0.001\n",
    "\n",
    "loss_set = []\n",
    "accu_set = []\n",
    "disp_freq = 100\n",
    "\n",
    "# Training process\n",
    "for epoch in range(0, max_epoch):\n",
    "    iter_per_batch = train_size // batch_size\n",
    "    for batch_id in range(0, iter_per_batch):\n",
    "        batch = train_set.next_batch(batch_size) # get data of next batch\n",
    "        input, label = batch\n",
    "        \n",
    "        # softmax_classifier\n",
    "        loss, gradient, prediction = softmax_classifier(W, input , label, lamda)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        label = np.argmax(label, axis=1) # scalar representation\n",
    "        accuracy = sum(prediction == label) / float(len(label))\n",
    "        \n",
    "        loss_set.append(loss)\n",
    "        accu_set.append(accuracy)\n",
    "        \n",
    "        # Update weights\n",
    "        W = W - (learning_rate * gradient)\n",
    "        if batch_id % disp_freq == 0:\n",
    "            print(\"Epoch [{}][{}]\\t Batch [{}][{}]\\t Training Loss {:.4f}\\t Accuracy {:.4f}\".format(\n",
    "                epoch, max_epoch, batch_id, iter_per_batch, \n",
    "                loss, accuracy))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "iter_per_batch = test_size // batch_size\n",
    "\n",
    "# Test process\n",
    "for batch_id in range(0, iter_per_batch):\n",
    "    batch = test_set.next_batch(batch_size)\n",
    "    data, label = batch\n",
    "    \n",
    "    # We only need prediction results in testing\n",
    "    _,_, prediction = softmax_classifier(W, data , label, lamda)\n",
    "    label = np.argmax(label, axis=1)\n",
    "    \n",
    "    correct += sum(prediction == label)\n",
    "    \n",
    "accuracy = correct * 1.0 / test_size\n",
    "print('Test Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training loss curve\n",
    "plt.figure()\n",
    "plt.plot(loss_set, 'b--')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "# training accuracy curve\n",
    "plt.figure()\n",
    "plt.plot(accu_set, 'r--')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have finished homework1-softmax, congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
