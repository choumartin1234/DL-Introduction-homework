{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework-2: ConvNet for MNIST Classification\n",
    "\n",
    "### **Deadline: 2019.11.03 23:59:59**\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement forward and backward for ConvLayer (`layers/conv_layer.py`)\n",
    "- #### implement forward and backward for PoolingLayer (`layers/pooling_layer.py`)\n",
    "- #### implement forward and backward for ReshapeLayer (`layers/reshape_layer.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chou\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import cupy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [1, 28, 28])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters\n",
    "You can modify hyperparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 2\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate = 0.15\n",
    "weight_decay = 0.005\n",
    "\n",
    "disp_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, ReLULayer, ConvLayer, MaxPoolingLayer, ReshapeLayer\n",
    "\n",
    "convNet = Network()\n",
    "# Build ConvNet with ConvLayer and PoolingLayer\n",
    "convNet.add(ConvLayer(1, 8, 3, 1))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(MaxPoolingLayer(2, 0))\n",
    "convNet.add(ConvLayer(8, 16, 3, 1))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(MaxPoolingLayer(2, 0))\n",
    "convNet.add(ReshapeLayer((batch_size, 16, 7, 7), (batch_size, 784)))\n",
    "convNet.add(FCLayer(784, 128))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][2]\t Batch [0][550]\t Training Loss 6.8729\t Accuracy 0.1200\n",
      "Epoch [0][2]\t Batch [10][550]\t Training Loss 8.9743\t Accuracy 0.1736\n",
      "Epoch [0][2]\t Batch [20][550]\t Training Loss 5.5160\t Accuracy 0.3267\n",
      "Epoch [0][2]\t Batch [30][550]\t Training Loss 4.0861\t Accuracy 0.4381\n",
      "Epoch [0][2]\t Batch [40][550]\t Training Loss 3.2892\t Accuracy 0.5154\n",
      "Epoch [0][2]\t Batch [50][550]\t Training Loss 2.7522\t Accuracy 0.5786\n",
      "Epoch [0][2]\t Batch [60][550]\t Training Loss 2.3760\t Accuracy 0.6261\n",
      "Epoch [0][2]\t Batch [70][550]\t Training Loss 2.0947\t Accuracy 0.6632\n",
      "Epoch [0][2]\t Batch [80][550]\t Training Loss 1.9089\t Accuracy 0.6847\n",
      "Epoch [0][2]\t Batch [90][550]\t Training Loss 1.7368\t Accuracy 0.7069\n",
      "Epoch [0][2]\t Batch [100][550]\t Training Loss 1.5926\t Accuracy 0.7280\n",
      "Epoch [0][2]\t Batch [110][550]\t Training Loss 1.4732\t Accuracy 0.7453\n",
      "Epoch [0][2]\t Batch [120][550]\t Training Loss 1.3726\t Accuracy 0.7598\n",
      "Epoch [0][2]\t Batch [130][550]\t Training Loss 1.2866\t Accuracy 0.7724\n",
      "Epoch [0][2]\t Batch [140][550]\t Training Loss 1.2135\t Accuracy 0.7818\n",
      "Epoch [0][2]\t Batch [150][550]\t Training Loss 1.1521\t Accuracy 0.7907\n",
      "Epoch [0][2]\t Batch [160][550]\t Training Loss 1.0962\t Accuracy 0.7987\n",
      "Epoch [0][2]\t Batch [170][550]\t Training Loss 1.0459\t Accuracy 0.8064\n",
      "Epoch [0][2]\t Batch [180][550]\t Training Loss 0.9982\t Accuracy 0.8140\n",
      "Epoch [0][2]\t Batch [190][550]\t Training Loss 0.9567\t Accuracy 0.8204\n",
      "Epoch [0][2]\t Batch [200][550]\t Training Loss 0.9193\t Accuracy 0.8263\n",
      "Epoch [0][2]\t Batch [210][550]\t Training Loss 0.8859\t Accuracy 0.8313\n",
      "Epoch [0][2]\t Batch [220][550]\t Training Loss 0.8559\t Accuracy 0.8364\n",
      "Epoch [0][2]\t Batch [230][550]\t Training Loss 0.8274\t Accuracy 0.8409\n",
      "Epoch [0][2]\t Batch [240][550]\t Training Loss 0.8038\t Accuracy 0.8441\n",
      "Epoch [0][2]\t Batch [250][550]\t Training Loss 0.7803\t Accuracy 0.8479\n",
      "Epoch [0][2]\t Batch [260][550]\t Training Loss 0.7572\t Accuracy 0.8518\n",
      "Epoch [0][2]\t Batch [270][550]\t Training Loss 0.7351\t Accuracy 0.8556\n",
      "Epoch [0][2]\t Batch [280][550]\t Training Loss 0.7145\t Accuracy 0.8591\n",
      "Epoch [0][2]\t Batch [290][550]\t Training Loss 0.6976\t Accuracy 0.8613\n",
      "Epoch [0][2]\t Batch [300][550]\t Training Loss 0.6802\t Accuracy 0.8643\n",
      "Epoch [0][2]\t Batch [310][550]\t Training Loss 0.6658\t Accuracy 0.8663\n",
      "Epoch [0][2]\t Batch [320][550]\t Training Loss 0.6497\t Accuracy 0.8690\n",
      "Epoch [0][2]\t Batch [330][550]\t Training Loss 0.6353\t Accuracy 0.8714\n",
      "Epoch [0][2]\t Batch [340][550]\t Training Loss 0.6211\t Accuracy 0.8737\n",
      "Epoch [0][2]\t Batch [350][550]\t Training Loss 0.6078\t Accuracy 0.8758\n",
      "Epoch [0][2]\t Batch [360][550]\t Training Loss 0.5954\t Accuracy 0.8779\n",
      "Epoch [0][2]\t Batch [370][550]\t Training Loss 0.5844\t Accuracy 0.8795\n",
      "Epoch [0][2]\t Batch [380][550]\t Training Loss 0.5737\t Accuracy 0.8811\n",
      "Epoch [0][2]\t Batch [390][550]\t Training Loss 0.5636\t Accuracy 0.8826\n",
      "Epoch [0][2]\t Batch [400][550]\t Training Loss 0.5530\t Accuracy 0.8845\n",
      "Epoch [0][2]\t Batch [410][550]\t Training Loss 0.5432\t Accuracy 0.8862\n",
      "Epoch [0][2]\t Batch [420][550]\t Training Loss 0.5361\t Accuracy 0.8872\n",
      "Epoch [0][2]\t Batch [430][550]\t Training Loss 0.5277\t Accuracy 0.8884\n",
      "Epoch [0][2]\t Batch [440][550]\t Training Loss 0.5194\t Accuracy 0.8900\n",
      "Epoch [0][2]\t Batch [450][550]\t Training Loss 0.5114\t Accuracy 0.8912\n",
      "Epoch [0][2]\t Batch [460][550]\t Training Loss 0.5030\t Accuracy 0.8926\n",
      "Epoch [0][2]\t Batch [470][550]\t Training Loss 0.4963\t Accuracy 0.8937\n",
      "Epoch [0][2]\t Batch [480][550]\t Training Loss 0.4892\t Accuracy 0.8949\n",
      "Epoch [0][2]\t Batch [490][550]\t Training Loss 0.4820\t Accuracy 0.8963\n",
      "Epoch [0][2]\t Batch [500][550]\t Training Loss 0.4760\t Accuracy 0.8972\n",
      "Epoch [0][2]\t Batch [510][550]\t Training Loss 0.4694\t Accuracy 0.8986\n",
      "Epoch [0][2]\t Batch [520][550]\t Training Loss 0.4633\t Accuracy 0.8995\n",
      "Epoch [0][2]\t Batch [530][550]\t Training Loss 0.4575\t Accuracy 0.9005\n",
      "Epoch [0][2]\t Batch [540][550]\t Training Loss 0.4523\t Accuracy 0.9013\n",
      "Epoch [0]\t Average training loss 0.4472\t Average training accuracy 0.9022\n",
      "Epoch [0]\t Average validation loss 0.1304\t Average validation accuracy 0.9626\n",
      "Epoch [1][2]\t Batch [0][550]\t Training Loss 0.1403\t Accuracy 0.9600\n",
      "Epoch [1][2]\t Batch [10][550]\t Training Loss 0.1321\t Accuracy 0.9564\n",
      "Epoch [1][2]\t Batch [20][550]\t Training Loss 0.1480\t Accuracy 0.9514\n",
      "Epoch [1][2]\t Batch [30][550]\t Training Loss 0.1375\t Accuracy 0.9565\n",
      "Epoch [1][2]\t Batch [40][550]\t Training Loss 0.1401\t Accuracy 0.9561\n",
      "Epoch [1][2]\t Batch [50][550]\t Training Loss 0.1368\t Accuracy 0.9575\n",
      "Epoch [1][2]\t Batch [60][550]\t Training Loss 0.1487\t Accuracy 0.9541\n",
      "Epoch [1][2]\t Batch [70][550]\t Training Loss 0.1467\t Accuracy 0.9546\n",
      "Epoch [1][2]\t Batch [80][550]\t Training Loss 0.1488\t Accuracy 0.9532\n",
      "Epoch [1][2]\t Batch [90][550]\t Training Loss 0.1474\t Accuracy 0.9541\n",
      "Epoch [1][2]\t Batch [100][550]\t Training Loss 0.1435\t Accuracy 0.9549\n",
      "Epoch [1][2]\t Batch [110][550]\t Training Loss 0.1479\t Accuracy 0.9539\n",
      "Epoch [1][2]\t Batch [120][550]\t Training Loss 0.1479\t Accuracy 0.9534\n",
      "Epoch [1][2]\t Batch [130][550]\t Training Loss 0.1464\t Accuracy 0.9541\n",
      "Epoch [1][2]\t Batch [140][550]\t Training Loss 0.1457\t Accuracy 0.9544\n",
      "Epoch [1][2]\t Batch [150][550]\t Training Loss 0.1452\t Accuracy 0.9542\n",
      "Epoch [1][2]\t Batch [160][550]\t Training Loss 0.1439\t Accuracy 0.9545\n",
      "Epoch [1][2]\t Batch [170][550]\t Training Loss 0.1458\t Accuracy 0.9546\n",
      "Epoch [1][2]\t Batch [180][550]\t Training Loss 0.1442\t Accuracy 0.9552\n",
      "Epoch [1][2]\t Batch [190][550]\t Training Loss 0.1443\t Accuracy 0.9551\n",
      "Epoch [1][2]\t Batch [200][550]\t Training Loss 0.1437\t Accuracy 0.9553\n",
      "Epoch [1][2]\t Batch [210][550]\t Training Loss 0.1424\t Accuracy 0.9558\n",
      "Epoch [1][2]\t Batch [220][550]\t Training Loss 0.1418\t Accuracy 0.9563\n",
      "Epoch [1][2]\t Batch [230][550]\t Training Loss 0.1402\t Accuracy 0.9570\n",
      "Epoch [1][2]\t Batch [240][550]\t Training Loss 0.1409\t Accuracy 0.9565\n",
      "Epoch [1][2]\t Batch [250][550]\t Training Loss 0.1406\t Accuracy 0.9567\n",
      "Epoch [1][2]\t Batch [260][550]\t Training Loss 0.1400\t Accuracy 0.9569\n",
      "Epoch [1][2]\t Batch [270][550]\t Training Loss 0.1413\t Accuracy 0.9566\n",
      "Epoch [1][2]\t Batch [280][550]\t Training Loss 0.1402\t Accuracy 0.9568\n",
      "Epoch [1][2]\t Batch [290][550]\t Training Loss 0.1404\t Accuracy 0.9568\n",
      "Epoch [1][2]\t Batch [300][550]\t Training Loss 0.1417\t Accuracy 0.9562\n",
      "Epoch [1][2]\t Batch [310][550]\t Training Loss 0.1418\t Accuracy 0.9561\n",
      "Epoch [1][2]\t Batch [320][550]\t Training Loss 0.1409\t Accuracy 0.9564\n",
      "Epoch [1][2]\t Batch [330][550]\t Training Loss 0.1408\t Accuracy 0.9564\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "convNet, conv_loss, conv_acc = train(convNet, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test(convNet, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_acc({'ConvNet': [conv_loss, conv_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have finished homework-2, congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
